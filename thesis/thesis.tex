\documentclass[12pt]{book}

\usepackage{suthesis-2e}
\usepackage{url}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage[inline]{enumitem}

\counterwithout{figure}{chapter}
\counterwithout{table}{chapter}


\begin{document}

    \title{Comparing Conditional Random Fields and LSTM Networks for Named Entity Recognition}
    \author{Josef Gugglberger}
    \principaladviser{Dipl.-Ing.Clemens Sauerwein, PhD}
 
    \beforepreface
    \prefacesection{Abstract} 

 	\afterpreface
 	
 	
    \chapter{Introduction}
    
    With the rise of deep learning in the last few years (from 2015 onwards), the methods to perform named entity recognition (NER) have changed in research. The traditional machine learning approaches like Hidden Markov Models and Conditional Random Fields (CRF) were replaced by a special type of Recurrent Neural Networks, so-called Long-Short-Term-Memory networks (LSTM). The first approach to apply LSTM networks for named entity recognition was done already in the year 2003 in the paper \cite{hammerton-2003-named}. However, due to the lack of computational power back then, the performance was far behind state-of-the-art methods. In 2015, Huang et al. \cite{huang2015bidirectional} showed various LSTM network that reached state-of-the-art performance. Also, the currently best performing NER system \cite{akbik2018contextual} on the CoNLL 2003 dataset makes use of some kind of LSTM network.\\
    
    Aim of this work is to perform a practical study, comparing the implementation and the performance of a machine learning approach, namely CRFs, and a deep learning approach implementing a LSTM network.\\
    
    The remainder of this thesis is structured as follows: Chapter \ref{chap:backround} is about related work and some background information on named entity recognition, CRFs and LSTM networks. Next, Chapter \ref{chap:method} describes the details of the two implementations of the NER system. Chapter \ref{chap:evaluation} compares the results generated by the previous chapter. Finally, Chapter \ref{chap:conclusion} concludes and give an outlook what can be done in future works.
   
	\chapter{Background \& Related Work}
	\label{chap:backround}
	
	This chapter presents basic background information of the topics NER (Section \ref{sec:ner}), CRF (Section \ref{sec:crf}) and LSTM networks (Section \ref{sec:lstm}). The last section, Section \ref{sec:relatedwork}, summarizes the recent work that was done in the area of NER. 

	
	\section{Named Entity Recognition}
	\label{sec:ner}
	
	\textit{Named Entity Recognition} is the task of locating and classifying named entities in unstructured text. A named entity is classified into a predefined set of categories, which can be names of people, locations, companies, etc. A named entity describes something physical (person, location, etc.), and a \textit{mention} is a phrase in the text that refers to that entity with a name \cite{MAL-013}. For example, the terms \textit{U.S.A} and \textit{the United States of America} are two different phrases that refer to the same named entity.
	
	The categories in which the named entities are classified depends strongly on the domain where the NER system will be deployed, but the categories \textit{person}, \textit{location}, \textit{organization} and \textit{miscellaneous} are often used in research, for example in \cite{tjongkimsang2003conll}.
	 
	According to AclWeb \cite{conll-sota}, the current state of the art approach on the CoNLL-2003 \cite{tjongkimsang2003conll} dataset reaches a F1 score of 93.09 \%.
	
	\section{Conditional Random Fields}
	\label{sec:crf}
	
	A \textit{Conditional Random Field}, initially proposed in \cite{lafferty2001conditional}, is a discriminative probabilistic classifier. In contrast to a discrete classifier (like the naive Bayes classifier), it makes its prediction not just based on the input sample, but also based on the context of the input sample. This is an important feature in the domain of natural language processing, especially in part-of-speech tagging and named entity recognition. 
	
	To model the context of a sample we have to define feature functions, which have to be defined in a way that expresses some characteristic that is present in the training data and we want our model to hold. A feature function has to form:
	
	\begin{equation}
	f(y_t, y_{t-1}, x_t)
	\end{equation}
	
	where $y_t$ is the current label, $y_{t-1}$ is the previous label and $x_t$ are the samples from the input sequence that the feature function needs to calculate the feature. The Conditional Random Field, as defined in \cite{MAL-013}, is then:
	
	\begin{equation}
	p(y|x) = \frac{1}{Z(x)} \prod_{t=1}^T exp(\sum_{k=1}^{K} \theta_k f_k(y_t, y_{t-1}, x_t))
	\end{equation}
	
	where $Z(x)$ is an normalization function:
	
	\begin{equation}
	Z(x) = \sum_{y} \prod_{t=1}^{T} exp(\sum_{k=1}^{K} \theta_k f_k(y_t, y_{t-1}, x_t))
	\end{equation}
	
	where $X$ and $Y$ are random vectors and $\theta$ is a parameter vector. The values of the parameter vector $\theta$ have to be learned from the training data, usually via Maximum Likelihood Estimation.
	
	\section{LSTM Networks}
	\label{sec:lstm}
	
	A LSTM network is a special type of Recurrent Neural Network (RNN). In contrast to a feed-forward neural network, like for example Convolutional Neural Network, where the date flows only in one direction (from input to output), the data can flow in cycles in a RNN. This property makes a RNN able to memorize inputs, which makes them suitable for sequence labeling tasks.
	RNNs have been considered difficult to train a long time, especially when they are trained to recognise long-term dependencies. The problems that arise in such cases is called the \textit{vanishing} and \textit{exploding} gradient problem \cite{lipton2015critical}. The vanishing gradient problem occurs because the gradients of back-propagation are multiplied by each other to get the dependencies from the previous steps.
	
	LSTM networks, proposed in \cite{hochreiter1997long}, were designed to overcome the issue that RNNs have with long term dependencies. Each LSTM cell holds an internal memory state, which is added (instead of multiplied) to the process input. The time dependence in the cell is determined by the forget gate.
	
	Figure \ref{img:lstm} shows a LSTM cell. The data flows from left to right. $x_t$ is the sequence at step t, $h_{t-1}$ is the output of the previous LSTM cell. The input $x_t$ and $h_{t-1}$ are concatenated at the first step, and is squashed between -1 and 1 by a $tanh$ function afterwards. The result of this is then multiplied element wise with the output of the input gate. The \textit{input} gate is sigmoid function, mapping inputs between 0 and 1, this can turn on and off parts of the input. 
	The next stage in the cell is called the \textit{forget} gate. The new variable $s$ represents the internal state of the LSTM cell. The internal state is delayed by one step and added to the output of the input gate. The sigmoid function determines which words can be forgotten (close to 0), and which words need to be remembered (output close to 1).
	Last stage is the \textit{output} gate, which operates in the same way as the input gate. The output is squashed between -1 and 1 by the hyperbolic tangent function, and the output gate with the sigmoid function determines which values to output.
	
	\begin{figure}
		\begin{center}
			\includegraphics[width=0.75\linewidth]{img/lstm.png}
		\end{center}
		\caption{Overview of a LSTM cell. Yellow nodes represent a hidden layer, red and green nodes represent a element-wise operation.}	
		\label{img:lstm}
	\end{figure}
	
	\section{Related Work}
	\label{sec:relatedwork}
	
	A comparison between CRF and LSTM networks was already proposed by Nikola Ljube{\v{s}}i{\'c} in \cite{ljubevsic2018comparing}. However, the analysis that was made there focuses on Slovene, Croatian and Serbian data.
	
	\chapter{Method}
	\label{chap:method}
	
	Aim of this work was to conduct a practical study comparing a CRF approach and a LSTM approach for NER. Two NER systems were developed, trained and tested by two different datasets. 
	
	\section{Dataset}
	
	Two very different datasets were used to train and test the NER systems. On the one hand the famous CoNLL 2003 \cite{sang2003introduction} dataset, which consists of newswire articles from the Reuters corpus, and on the other hand the W-NUT 17 \cite{derczynski2017results} dataset, which consists of noisy user generated text, were used.
	
	\section{Implementation}
	
	The following section presents implementation details of the developed NER systems. Subsection \ref{sub:CRF} starts with presenting the CRF approach. Second, Subsection \ref{sub:lstm} continues with the details of the LSTM approaches.
	
	\subsection{Conditional Random Fields}
	\label{sub:CRF}
	
	The CRF approach was implemented with the help of following Python libraries: \textit{python-crfsuit} \cite{pycrfsuite}, \textit{nltk} \cite{nltk} and \textit{gensim} \cite{gensim}.
	
	A lot of effort was invested to came up with a good working feature set. The employed feature function can be grouped into four categories: \begin{enumerate*}
		\item word-based features,
		\item sentence \& collection-based features,
		\item dictionary-based features, and
		\item features from unsupervised machine learning algorithms.
	\end{enumerate*}

	\subsubsection{Word-based features}
	
	Following characteristics of words were defined as word-based features:
	\begin{itemize}
		\item length of word
		\item word starts with upper-case letter
		\item word contains upper-case letter
		\item word contains a digit
		\item word contains a special character
		\item shape of the word ('Word' maps to 'Aa+', 'WORD' maps to 'A+', 2020-01-16 maps to 9999\#99\#99)
	\end{itemize}
	
	\subsubsection{Sentence \& collection-based features}
	
	Following characteristics of were defined as sentence and collection-based features:
	\begin{itemize}
		\item position of word in sentence
		\item number of occurrences in collection
	\end{itemize}
	
	\subsubsection{Dictionary-based features}
	
	For gathering dictionary features each word was looked up in multiple dictionaries. Following dictionaries where used:
	\begin{itemize}
		\item stop-words list by nltk corpus
		\item name-list by nltk corpus
		\item word-list by nltk corpus
		\item Length of hypernym path in WordNet \cite{wordnet}. This gives some kind of information like the specificness or rareness of a word.
	\end{itemize}
	
	\subsubsection{Features from unsupervised machine learning algorithms}
	Several features were collected from unsupervised ML clustering algorithms. The cluster which the word appears in is used as a feature value.
	Following algorithms were used:
	\begin{itemize}
		\item Brown cluster \cite{brown1992class}. The cluster where the word is in and the bit-sequence returned from the algorithm is used as a feature. The best results were achieved with five clusters.
		\item Latent Dirichlet Allocation (LDA) \cite{blei2003latent}. LDA is used to model the abstract topic of a document. It returns a distribution of topics per word. The most dominant topic of each word was used as a feature.
		\item Word2Vec cluster \cite{w2vgensim}. Word2Vec learns relationships between words, and outputs one vector for each word. It maps similar words to similar vectors. The vector is used as a feature.
	\end{itemize}
	
	\subsection{LSTM Network}
	\label{sub:lstm}
	
	For the implementation of the LSTM networks the Python deep learning package Keras \cite{keras} was used with TensorFlow \cite{tensorflow} as a backend. The two implemented approaches where first proposed by Huang et al. in \cite{huang2015bidirectional}. Both approaches use a bidirectional LSTM layer, which means that the LSTM network is duplicated. The input as-is is feed into the first layer, the input reversed is feed into the second layer. This can be beneficial, because to classifying a named entity, contextual information about past (words the left of current) and future (words on the right of current) is needed.
	
	The Keras model of the BI-LSTM approach is shown in figure \ref{fig:lstm}. After the input layer, strings are converted to vectors in the embedding layer. The embedding layer was pre-trained with GloVe \cite{glove} embeddings with a vector size of 100. After embedding the words, a Dropout layer is applied to avoid over-fitting. The theory behind the Dropout layer was proposed by Srivastava et al. in \cite{srivastava2014dropout}. Key idea is to randomly drop a defined amount of units in training. Next, the data flows into the bidirectional LSTM layer. A time distributed dense layer is than applied before output. A time distributed dense layer is used here instead of a normal dense layer because of the sequential nature of natural language: there is one output (named entity label) per time-step.

	\begin{figure}
		\begin{center}
			\includegraphics[width=0.4\linewidth]{img/lstm_model.png}
		\end{center}
		\caption{Keras model of NER system with Bidirectional-LSTM network.}
		\label{fig:lstm}
	\end{figure}

	The Keras model of the BI-LSTM-CRF approach is shown in figure \ref{fig:lstm}. It is the same model as described in the previous paragraph, with an additional CRF layer added at the bottom. Basically it is a combination of the first presented CRF approach and the BI-LSTM approach presented above. The CRF layer has the advantage that it does also capture sentence level tag information, instead of just use past and future features as the BI-LSTM approach.
	
	\begin{figure}
		\begin{center}
			\includegraphics[width=0.4\linewidth]{img/lstm_crf_model.png}
		\end{center}
		\caption{Keras model of NER system with Bidirectional-LSTM-CRF network.}
		\label{fig:lstm-crf}
	\end{figure}
	

	\chapter{Evaluation \& Comparison}
	\label{chap:evaluation}
	
	Table \ref{tab:res_conll} and \ref{tab:res_wnut} display the results of the various implemented tools, together with a state-of-the-art approach called \textit{Flair} \cite{akbik2019flair}. Evaluation was done, as described in \cite{tjongkimsang2003conll}, by marking predicted named entities as correct \textit{iff} it is an \textit{exact} match with the test data file. The implemented approaches where trained with a train test split of 85\% / 15\%, the results are therefore not completely comparable with other participants of the CoNLL and W-NUT competition.
	
	Table \ref{tab:res_conll} shows precision, recall and F1-score on the CoNLL 2003 dataset. The general observing of this table is that the deep learning approaches Bi-LSTM and Bi-LSTM-CRF performed better than the machine learning approach CRF on all metrics.
	
	Table \ref{tab:res_wnut} show precision, recall and F1-score on the W-NUT 17 dataset. Key finding here is that the CRF approaches CRF and Bi-LSTM-CRF performed much better than the non-CRF approach Bi-LSTM.
	
	Both tables also show the state-of-the-art approach called \textit{Flair}. Flair also uses a BI-LSTM-CRF model, but it was implemented with the deep learning library PyTorch \cite{pytorch} instead of Keras. What makes their approach different is a new type of string embedding, proposed by Akbik et al. in \cite{akbik2018contextual}.
	
	\begin{table}
		\begin{center}
			\begin{tabular}{| l | c | c | c |}
				\hline
				Method & Precision & Recall & F1-score \\ \hline
				CRF & 84.25 & 85.42 & 84.83 \\ \hline
				Bi-LSTM & 84.37 & \textbf{86.58} & 85.46 \\ \hline
				Bi-LSTM-CRF & \textbf{89.41} & 86.20 & \textbf{87.78} \\ \hline
				Flair & - & - & 93.09 \\ \hline
			\end{tabular}
		\end{center}
		\caption{Results on CoNLL 2003 dataset.}
		\label{tab:res_conll}
	\end{table}
	\begin{table}
		\begin{center}
			\begin{tabular}{| l | c | c | c |}
				\hline
				Method & Precision & Recall & F1-score \\ \hline
				CRF & 31.54 & \textbf{56.72} & \textbf{40.53} \\ \hline
				Bi-LSTM & 8.69 & 23.16 & 12.63 \\ \hline
				Bi-LSTM-CRF & \textbf{33.61} & 31.03 & 32.27 \\ \hline
				Flair & - & - & 49.49 \\ \hline
			\end{tabular}
		\end{center}
		\caption{Results on W-NUT 17 dataset.}
		\label{tab:res_wnut}
	\end{table}

	\chapter{Conclusion}
	\label{chap:conclusion}
	
	\bibliographystyle{plain}
	\bibliography{thesis}

\end{document}
