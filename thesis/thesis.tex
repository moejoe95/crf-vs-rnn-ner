\documentclass[12pt]{book}

\usepackage{suthesis-2e}
\usepackage{url}

\counterwithout{figure}{chapter}
\counterwithout{table}{chapter}


\begin{document}

    \title{Comparing Conditional Random Fields and LSTM Networks for Named Entity Recognition}
    \author{Josef Gugglberger}
    \principaladviser{Dipl.-Ing.Clemens Sauerwein, PhD}
 
    \beforepreface
    \prefacesection{Abstract} 

 	\afterpreface
 	
 	
    \chapter{Introduction}
    
    With the rise of deep learning in the last few years (from 2015 onwards), the methods to perform named entity recognition (NER) have changed in research. The traditional machine learning approaches like Hidden Markov Models and Conditional Random Fields (CRF) were replaced by a special type of Recurrent Neural Networks, so-called Long-Short-Term-Memory networks (LSTM). The first approach to apply LSTM networks for named entity recognition was done already in the year 2003 in the paper \cite{hammerton-2003-named}. However, due to the lack of computational power back then, the performance was far behind state-of-the-art methods. In 2015, Huang et al. \cite{huang2015bidirectional} showed various LSTM network that reached state-of-the-art performance. Also, the currently best performing NER system \cite{akbik2018contextual} on the CoNLL 2003 dataset makes use of some kind of LSTM network.\\
    
    Aim of this work is to perform a practical study, comparing the implementation and the performance of a machine learning approach, namely CRFs, and a deep learning approach implementing a LSTM network.\\
    
    The remainder of this thesis is structured as follows: Chapter \ref{chap:backround} is about related work and some background information on named entity recognition, CRFs and LSTM networks. Next, Chapter \ref{chap:method} describes the details of the two implementations of the NER system. Chapter \ref{chap:evaluation} compares the results generated by the previous chapter. Finally, Chapter \ref{chap:conclusion} concludes and give an outlook what can be done in future works.
   
	\chapter{Background \& Related Work}
	\label{chap:backround}
	
	This chapter presents basic background information of the topics NER (Section \ref{sec:ner}), CRF (Section \ref{sec:crf}) and LSTM networks (Section \ref{sec:lstm}). The last section, Section \ref{sec:relatedwork}, summarizes the recent work that was done in the area of NER. 

	
	\section{Named Entity Recognition}
	\label{sec:ner}
	
	\textit{Named Entity Recognition} is the task of locating and classifying named entities in unstructured text. A named entity is classified into a predefined set of categories, which can be names of people, locations, companies, etc. A named entity describes something physical (person, location, etc.), and a \textit{mention} is a phrase in the text that refers to that entity with a name \cite{MAL-013}. For example, the terms \textit{U.S.A} and \textit{the United States of America} are two different phrases that refer to the same named entity.
	
	The categories in which the named entities are classified depends strongly on the domain where the NER system will be deployed, but the categories \textit{person}, \textit{location}, \textit{organization} and \textit{miscellaneous} are often used in research, for example in \cite{tjongkimsang2003conll}.
	 
	According to AclWeb \cite{conll-sota}, the current state of the art approach on the CoNLL-2003 \cite{tjongkimsang2003conll} dataset reaches a F1 score of 93.09 \%.
	
	\section{Conditional Random Fields}
	\label{sec:crf}
	
	A \textit{Conditional Random Field}, initially proposed in \cite{lafferty2001conditional}, is a discriminative probabilistic classifier. In contrast to a discrete classifier (like the naive Bayes classifier), it makes its prediction not just based on the input sample, but also based on the context of the input sample. This is an important feature in the domain of natural language processing, especially in part-of-speech tagging and named entity recognition. 
	
	To model the context of a sample we have to define feature functions, which have to be defined in a way that expresses some characteristic that is present in the training data and we want our model to hold. A feature function has to form:
	
	\begin{equation}
	f(y_t, y_{t-1}, x_t)
	\end{equation}
	
	where $y_t$ is the current label, $y_{t-1}$ is the previous label and $x_t$ are the samples from the input sequence that the feature function needs to calculate the feature. The Conditional Random Field, as defined in \cite{MAL-013}, is then:
	
	\begin{equation}
	p(y|x) = \frac{1}{Z(x)} \prod_{t=1}^T exp(\sum_{k=1}^{K} \theta_k f_k(y_t, y_{t-1}, x_t))
	\end{equation}
	
	where $Z(x)$ is an normalization function:
	
	\begin{equation}
	Z(x) = \sum_{y} \prod_{t=1}^{T} exp(\sum_{k=1}^{K} \theta_k f_k(y_t, y_{t-1}, x_t))
	\end{equation}
	
	where $X$ and $Y$ are random vectors and $\theta$ is a parameter vector. The values of the parameter vector $\theta$ have to be learned from the training data, usually via Maximum Likelihood Estimation.
	
	\section{LSTM Networks}
	\label{sec:lstm}
	
	A LSTM network is a special type of Recurrent Neural Network (RNN). In contrast to a feed-forward neural network, like for example Convolutional Neural Network, where the date flows only in one direction (from input to output), the data can flow in cycles in a RNN. This property makes a RNN able to memorize inputs, which makes them suitable for sequence labeling tasks.
	RNNs have been considered difficult to train a long time, especially when they are trained to recognise long-term dependencies. The problems that arise in such cases is called the \textit{vanishing} and \textit{exploding} gradient problem \cite{lipton2015critical}.
	
	LSMT networks, proposed in \cite{hochreiter1997long}, were designed to overcome the issue mentioned in the above paragraph.
	
	\section{Related Work}
	\label{sec:relatedwork}
	
	$h_{t-1}$
	
	\chapter{Method}
	\label{chap:method}
	
	\section{Dataset}
	
	\section{Implementation}
	
	\subsection{Conditional Random Fields}
	
	\subsection{BI-LSTM Network}
	
	\subsection{BI-LSTM-CRF Network}

	\chapter{Evaluation \& Comparison}
	\label{chap:evaluation}
	
	\section{Results}

	\chapter{Conclusion}
	\label{chap:conclusion}
	
	\bibliographystyle{plain}
	\bibliography{thesis}

\end{document}
