\documentclass[12pt]{book}

\usepackage{suthesis-2e}

\counterwithout{figure}{chapter}
\counterwithout{table}{chapter}


\begin{document}

    \title{Comparison between Conditional Random Fields and LSTM Neural Network Approaches for Named Entity Recognition}
    \author{Josef Gugglberger}
    \principaladviser{Dipl.-Ing.Clemens Sauerwein, PhD}
 
    \beforepreface
    \prefacesection{Abstract} 

 	\afterpreface
 	
 	
    \chapter{Introduction}
   
	\chapter{Background \& Related Work}
	
	\section{Background}
	
	\subsection{Named Entity Recognition}
	
	\textit{Named Entity Recognition} is the task of locating and classifying named entities in unstructured text. A named entity is classified into a predefined set of categories, which can be names of people, locations, companies, etc. A named entity describes something physical (person, location, etc.), and a \textit{mention} is a phrase in the text that refers to that entity with a name \cite{MAL-013}. For example, the terms \textit{U.S.A} and \textit{United States of America} are two different phrases that refer to the same named entity.
	
	The categories in which the named entities are classified, depends strongly on the domain where the NER system will be deployed, but the categories \textit{person}, \textit{location}, \textit{organization} and \textit{miscellaneous} are often used in research, as for example in \cite{tjongkimsang2003conll}.
	 
	According to AclWeb \cite{conll-sota}, the current state of the art approach on the CoNLL-2003 \cite{tjongkimsang2003conll} dataset reaches a F1 score of 93.09 \%.
	
	\subsection{Conditional Random Fields}
	
	A \textit{Conditional random field}, initially proposed in \cite{lafferty2001conditional}, is a discriminative probabilistic classifier. In contrast to a discrete classifier (like the naive Bayes classifier), it makes it's prediction not just based on the input sample, but also based on the context of the input sample. This is a important feature in the domain of natural language processing, especially in part-of-speech tagging and named entity recognition. 
	
	To model the context of a sample we have to define feature functions, which have to be defined in way that express some characteristic that is present in the training data and we want our model to hold. A feature function has to form:
	
	\begin{equation}
	f(y_t, y_{t-1}, x_t)
	\end{equation}
	
	where $y_t$ is the current label, $y_{t-1}$ is the previous label and $x_t$ are the samples from the input sequence that the feature function needs to calculate the feature. The conditional random field, as defined in \cite{MAL-013}, is then:
	
	\begin{equation}
	p(y|x) = \frac{1}{Z(x)} \prod_{t=1}^T exp(\sum_{k=1}^{K} \theta_k f_k(y_t, y_{t-1}, x_t))
	\end{equation}
	
	where $Z(x)$ is an normalization function:
	
	\begin{equation}
	Z(x) = \sum_{y} \prod_{t=1}^{T} exp(\sum_{k=1}^{K} \theta_k f_k(y_t, y_{t-1}, x_t))
	\end{equation}
	
	where $X$ and $Y$ are random vectors and $\theta$ is a parameter vector. The values of the parameter vector $\theta$ have to be learned from the training data, usually via Maximum Likelihood Estimation.
	
	\subsection{LSTM Neural Networks}
	
	\section{Related Work}

	\chapter{Method}
	
	\section{Dataset}
	
	\section{Implementation}
	
	\subsection{Conditional Random Fields}
	
	\subsection{BI-LSTM Network}
	
	\subsection{BI-LSTM-CRF Network}

	\chapter{Evaluation \& Comparison}
	
	\section{Results}

	\chapter{Conclusion}
	
	\bibliographystyle{plain}
	\bibliography{thesis}

\end{document}
