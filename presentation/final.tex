\documentclass[12pt, tikz]{beamer}

\usetheme[progressbar=frametitle]{metropolis}
\usepackage{appendixnumberbeamer}
\usepackage{graphicx,wrapfig}
\usepackage[export]{adjustbox}
\usepackage{pgfplots}
\usepackage{verbatim}
\usepackage{graphicx}
\usepackage[backend=bibtex, style=authoryear]{biblatex}
\bibliography{thesis}
\usepackage{tikz}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{caption}
\usepackage[normalem]{ulem}
\usepackage{dirtree}
\usetikzlibrary{graphdrawing}
\usetikzlibrary{graphs}
\usepackage{subcaption}

\setbeamercolor{block title}{bg=gray!50}
\setbeamercolor{block body}{bg=gray!30}
\setbeamercolor{block title example}{fg={rgb:green,1;black,2}}
\setbeamercolor{block body example}{fg={rgb:green,1;black,2}}

\setbeamercolor{background canvas}{bg=white}

\definecolor{darkblue}{rgb}{0.0,0.0,0.6}
\definecolor{bracket}{RGB}{130,130,130}
\definecolor{quote}{RGB}{150,50,50}

\lstset{
  basicstyle=\ttfamily\scriptsize,
  columns=fullflexible,
  showstringspaces=false,
  commentstyle=\color{gray}\upshape
}

\usepackage[most]{tcolorbox}

\setbeamertemplate{footline}[text line]{
	\parbox{\linewidth}{\vspace*{-8pt} \hfill\insertpagenumber}}
\setbeamertemplate{navigation symbols}{}

\title{Comparing Conditional Random Fields and LSTM Networks for Named Entity Recognition}	
\date{\today}

\author{Josef Gugglberger (student)
\and Clemens Sauerwein (supvervisor)}
\institute{LV 703605-6 Masterseminar
\and \includegraphics[scale=0.3]{img/universitaet-innsbruck-logo-rgb-farbe}}

\begin{document}
	
% titel
\begin{frame}
\maketitle
\end{frame}

\begin{frame}[fragile]{Motivation}
	test
\end{frame}

% Ã¼berblick
\begin{frame}{Overview}
\setbeamertemplate{section in toc}[sections numbered]
\tableofcontents[hideallsubsections]
\end{frame}


\section{Background \& Related Work}

\subsection{Background}

\begin{frame}[fragile]{Named Entity Recognition}
	\begin{block}{Definition: NER}
		\textit{Named Entity Recognition} is the task of locating and classifying named entities in unstructured text. A named entity is classified into a predefined set of categories.
	\end{block}
	\pause
	
	\begin{center}
		
	James visited the Eiffel Tower in 2012.\\
	$\downarrow$ \\
	\textcolor{red}{James} \scalebox{0.4}{[PERSON]} visited the \textcolor{green}{Eiffel} \scalebox{0.4}{[LOCATION]} \textcolor{green}{Tower} \scalebox{0.4}{[LOCATION]} in \textcolor{blue}{2012} \scalebox{0.4}{[TIME]}.
	
	\end{center}
\end{frame}


\begin{frame}[fragile]{Conditional Random Fields}
	\begin{block}{Definition: CRF}
	A \textit{Conditional Random Field} is a discriminative probabilistic classifier. It makes its prediction not just based on the input sample, but also based on the context of the input sample.
	\end{block}
	\pause
	\vspace{-0.35cm}
	\begin{equation}
	p(y|x) = \frac{1}{Z(x)} \prod_{t=1}^T exp(\sum_{k=1}^{K} \theta_k f_k(y_t, y_{t-1}, x_t))
	\end{equation}

	where $Z(x)$ is an normalization function:

	\begin{equation}
	Z(x) = \sum_{y} \prod_{t=1}^{T} exp(\sum_{k=1}^{K} \theta_k f_k(y_t, y_{t-1}, x_t))
	\end{equation}
\end{frame}

\begin{frame}[fragile]{Recurrent Neural Networks}
	
	\begin{block}{Definition: RNN}
		RNNs are a special type of artificial neural networks that have a feedback loop feeding the hidden layers back into themselves. The loop provides a kind of memory that allow the network to better recognize patterns.
	\end{block}
	\pause
	\begin{itemize}
		\item Suited for sequence labeling
		\item Problems with long term dependencies
		\item Vanishing and exploding gradient
	\end{itemize}
\end{frame}

\begin{frame}[fragile]{Long-Short-Term-Memory Networks}

	\begin{block}{Definition: LSTM networks}
		LSTM networks are a special case of RNNs, which where designed to overcome issues with the vanishing gradient on long term relationships.
	\end{block}
	\pause
	\vspace{-0.35cm}
	\begin{figure}
		\includegraphics[width=\linewidth]{img/lstm.png}
	\end{figure}
\end{frame}

\section{Implementation Details}

\begin{frame}[fragile]{Dataset CoNLL}
	
	\begin{block}{Conference on Computational NL Learning}
		CoNLL 2003 was a shared task on language independent named entity recognition. The data is based on news wire articles from the Reuters corpus.
	\end{block}

	Four types of Named Entities:
	\begin{itemize}
		\item Person
		\item Location
		\item Organization
		\item Miscellaneous
	\end{itemize}

\end{frame}

\begin{frame}[fragile]{Dataset W-NUT}
	
	\begin{block}{Workshop on Noisy User-generated Text}
		W-NUT 17 was a workshop that focused on NLP on noisy and informal text, such as comments from social media, online reviews, forums, etc.
	\end{block}
	
	Four types of Named Entities:
	\begin{itemize}
		\item Person
		\item Location
		\item Corporation
		\item Consumer good
		\item Creative work
		\item Group 
	\end{itemize}
	
\end{frame}

\begin{frame}[fragile]{Dataset Syntax}
	\begin{center}
		\begin{tabular}{l | l | l | l}
			Word & POS & Syntax Chunk & NE \\
			\hline
			U.N. & NNP & I-NP & I-ORG 
\\
			official & NN & I-NP & O 
\\
			Ekeus & NNP & I-NP & I-PER \\ 
			heads & VBZ & I-VP & O 
\\
			for & IN & I-PP & O \\
			Baghdad & NNP & I-NP & I-LOC \\ 
			. & . & O & O \\
		\end{tabular}
	\end{center}
\end{frame}

\begin{frame}[fragile]{Conditional Random Fields}
	Libraries:
	\begin{itemize}
		\item pycrfsuite
		\item nltk
		\item gensim
	\end{itemize}

	Features should describe characteristics of named entities.
	
	\begin{itemize}
		\item Word Features
		\item Sentence \& Collection Features
		\item Dictionary Features
		\item Features from unsupervised ML algorithms
	\end{itemize}
\end{frame}

\begin{frame}[fragile]{Word Features}
	\begin{itemize}
		\item length of word
		\item the word starts with an upper-case letter
		\item the word contains an upper-case letter
		\item the word contains a digit
		\item the word contains a special character (-, /, etc.)
		\item word shape: 'Word' $\rightarrow$ 'Aa+', 'WORD' $\rightarrow$ 'A+', '2019-12-12' $\rightarrow$ '9999\#99\#99'
	\end{itemize}
\end{frame}

\begin{frame}[fragile]{Sentence \& Collection Features}
	\begin{itemize}
		\item position of word in sentence
		\item number of occurrences in collection
	\end{itemize}
\end{frame}

\begin{frame}[fragile]{Dictionary Features}
	The word is contained in:
	\begin{itemize}
		\item \textbf{stop-words} list
		\begin{itemize}
			\item is, as, the, are, has, that, etc.
			\item Problems: 'The Who', 'Take That'
		\end{itemize}
		\item \textbf{name list}
		\begin{itemize}
			\item 7579 person names form nltk corpus
		\end{itemize}
		\item \textbf{word list}
		\begin{itemize}
			\item dictionary of 235892 words from nltk corpus
		\end{itemize}
		\item \textbf{wordnet}
		\begin{itemize}
			\item dictionary and thesaurus
			\item provides hypernyms, synonyms, etc.
		\end{itemize}
	\end{itemize}
\end{frame}

\begin{frame}[fragile]{Features from unsupervised ML algorithms}
	The cluster of each word is used as a feature.
	\begin{itemize}
		\item \textbf{brown cluster}
		\begin{itemize}
			\item hierarchical clustering algorithm
		\end{itemize}		
	\end{itemize}
	\begin{center}
		\includegraphics[width=0.5\linewidth]{img/brown.png}
	\end{center}
	
\end{frame}

\begin{frame}[fragile]{Features from unsupervised ML algorithms}
	The cluster of each word is used as a feature.
	\begin{itemize}
		\item<1-> \textbf{brown cluster}
		\begin{itemize}
			\item hierarchical clustering algorithm
		\end{itemize}
		\item<1-> \textbf{Latent Dirichlet Allocation} (LDA) topic
		\begin{itemize}
			\item modelling the abstract topics of document
			\item example: document A is 20\% topic 1, 60\% topic 2 and 20\% topic 3
		\end{itemize}
		\item<2-> gensim implementation of \textbf{w2v cluster}
		\begin{itemize}
			\item maps similar words to similar vectors
			\item $w2v(king) - w2v(man) + w2v(woman) = \sim w2v(queen)$
		\end{itemize}
	\end{itemize}
\end{frame}

\begin{frame}[fragile]{LSTM Network}
	Libraries:
	\begin{itemize}
		\item Keras functional API
		\item Tensorflow as backend
	\end{itemize}
	\pause
	\begin{figure}
	\includegraphics[width=0.6\textheight]{img/lstm_model.png}
	\end{figure}
\end{frame}

\begin{frame}[fragile]{Bidirectional LSTM Layer}
	\begin{itemize}
		\item idea is to duplicate LSTM layer
		\begin{itemize}
			\item input as-is is feed into first LSTM layer
			\item input reversed is feed into second LSTM layer
		\end{itemize}
		\item speech depends on context past and future
	\end{itemize}

	\pause

	Example:

	The other day we saw Paris \uncover<2>{.} \uncover<3->{\textcolor{red}{Hilton.}} 
\end{frame}

\begin{frame}[fragile]{Time Distributed Dense Layer}
	\begin{itemize}
		\item adds the same dense layer to every timestep 
	\end{itemize}
	\pause
	\begin{figure}
		\includegraphics[width=\linewidth]{img/lstm_overview.png}
	\end{figure}
\end{frame}

\begin{frame}[fragile]{The best of both worlds?}
	combine the LSTM approach with CRF by adding a CRF layer at bottom:
	\begin{itemize}
		\item use past input features via LSTM layer
		\item use sentence level tag information via CRF layer
	\end{itemize}
	\pause
	\vspace{-0.2cm}
	
	\centering
	\begin{figure}
		\centering
		\begin{subfigure}{.5\textwidth}
			\centering
			\includegraphics[width=\linewidth]{img/lstm_crf_model.png}
		\end{subfigure}%
		\begin{subfigure}{.5\textwidth}
			\centering
			\includegraphics[width=\linewidth]{img/lstm_crf_overview.png}
		\end{subfigure}
		
	\end{figure}

	
\end{frame}


\section{Evaluation and Comparison}

\begin{frame}[fragile]{Evaluation}
	Evaluation of the implemented NER systems with metrics:
	\begin{itemize}
		\item accuracy
		\item recall
		\item F1-score
	\end{itemize}

	\pause
	
	Evaluation performed based on:
	\begin{itemize}
		\item token level
		\item named entity level (CoNLL standard)
	\end{itemize}
\end{frame}

\begin{frame}[fragile]{Results CoNLL Dataset}
	token based:
	\begin{center}
		\begin{tabular}{| l | c | c | c |}
			\hline
			Method & Accuracy & Recall & F1-score \\ \hline
			CRF &  &  &  \\ \hline
			Bi-LSTM &  &  &  \\ \hline
			Bi-LSTM-CRF &  &  &  \\ \hline
		\end{tabular}
	\end{center}

	named entity based:
	\begin{center}
		\begin{tabular}{| l | c | c | c |}
			\hline
			Method & Accuracy & Recall & F1-score \\ \hline
			CRF &  &  &  \\ \hline
			Bi-LSTM &  &  &  \\ \hline
			Bi-LSTM-CRF &  &  &  \\ \hline
		\end{tabular}
	\end{center}
\end{frame}

\begin{frame}[fragile]{Results WNUT Dataset}
	token based:
	\begin{center}
		\begin{tabular}{| l | c | c | c |}
			\hline
			Method & Accuracy & Recall & F1-score \\ \hline
			CRF &  &  &  \\ \hline
			Bi-LSTM &  &  &  \\ \hline
			Bi-LSTM-CRF &  &  &  \\ \hline
		\end{tabular}
	\end{center}
	
	named entity based:
	\begin{center}
		\begin{tabular}{| l | c | c | c |}
			\hline
			Method & Accuracy & Recall & F1-score \\ \hline
			CRF &  &  &  \\ \hline
			Bi-LSTM &  &  &  \\ \hline
			Bi-LSTM-CRF &  &  &  \\ \hline
		\end{tabular}
	\end{center}
\end{frame}


\section{Conclusion}


%\printbibliography

\end{document}